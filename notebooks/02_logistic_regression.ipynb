{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — Logistic Regression (TF-IDF & Word2Vec)\n",
    "\n",
    "This notebook trains two logistic regression classifiers for fake news detection:\n",
    "\n",
    "1. **TF-IDF** — sparse bag-of-words / bigram features  \n",
    "2. **Word2Vec** — dense semantic embeddings (Google News, 300d)  \n",
    "\n",
    "Both models are evaluated with standard metrics, cross-validation, and SHAP explainability. Trained artifacts are saved to `models/`.\n",
    "\n",
    "**Outline**\n",
    "1. Load data  \n",
    "2. Train/test split  \n",
    "3. TF-IDF Logistic Regression — metrics, SHAP, save  \n",
    "4. Word2Vec Logistic Regression — metrics, save  \n",
    "5. 5-fold cross-validation (both models)  \n",
    "6. Error analysis (TF-IDF model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "MODELS_DIR = '../models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print('Libraries loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/cleaned_isot.csv')\n",
    "print(f'Loaded {len(df):,} rows')\n",
    "print(df['class'].value_counts())\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Train / Test Split (80/20, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text'].values\n",
    "y = df['class'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Train size : {len(X_train):,}')\n",
    "print(f'Test size  : {len(X_test):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. TF-IDF Logistic Regression\n",
    "\n",
    "We use `TfidfVectorizer` with up to 50,000 unigram + bigram features. The vectorizer is **fit only on the training set** to prevent data leakage, then used to transform the test set.\n",
    "\n",
    "`LogisticRegression` with `max_iter=1000` is then trained on the resulting sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=50_000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf  = tfidf.transform(X_test)\n",
    "\n",
    "# Train model\n",
    "lr_tfidf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_tfidf = lr_tfidf.predict(X_test_tfidf)\n",
    "y_prob_tfidf = lr_tfidf.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "print('TF-IDF Logistic Regression Results')\n",
    "print(f\"  Accuracy  : {accuracy_score(y_test, y_pred_tfidf):.4f}\")\n",
    "print(f\"  Precision : {precision_score(y_test, y_pred_tfidf):.4f}\")\n",
    "print(f\"  Recall    : {recall_score(y_test, y_pred_tfidf):.4f}\")\n",
    "print(f\"  F1        : {f1_score(y_test, y_pred_tfidf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_tfidf)\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "ax.set_xlabel('Predicted'); ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix — TF-IDF LR', fontweight='bold')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. ROC Curve & Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_prob_tfidf)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_prob_tfidf)\n",
    "pr_auc = auc(rec, prec)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(fpr, tpr, lw=2, label=f'AUC = {roc_auc:.4f}')\n",
    "ax1.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "ax1.set_xlabel('False Positive Rate'); ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve — TF-IDF LR', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(rec, prec, lw=2, label=f'PR-AUC = {pr_auc:.4f}')\n",
    "ax2.set_xlabel('Recall'); ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision-Recall Curve — TF-IDF LR', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. SHAP Explainability\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) decomposes each prediction into the contributions of individual features. With a linear model we use `shap.LinearExplainer` which is exact and fast.\n",
    "\n",
    "The **summary bar plot** shows globally which words most influence fake vs. real predictions. The **waterfall plots** show individual article decisions, making model reasoning transparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SHAP explainer\n",
    "explainer = shap.LinearExplainer(lr_tfidf, X_train_tfidf, feature_perturbation='interventional')\n",
    "\n",
    "# Compute SHAP values for the test set (use a sample for speed)\n",
    "sample_idx = np.random.RandomState(42).choice(X_test_tfidf.shape[0], size=2000, replace=False)\n",
    "X_sample = X_test_tfidf[sample_idx]\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(f'SHAP values computed for {X_sample.shape[0]} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary bar plot — top 20 most important words/bigrams\n",
    "shap.summary_plot(\n",
    "    shap_values, X_sample,\n",
    "    feature_names=feature_names,\n",
    "    max_display=20,\n",
    "    plot_type='bar',\n",
    "    show=False\n",
    ")\n",
    "plt.title('SHAP Feature Importance — TF-IDF LR (top 20)', fontweight='bold')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot — one correctly classified FAKE article\n",
    "fake_indices = np.where(y_test[sample_idx] == 0)[0]\n",
    "correct_fake = fake_indices[y_pred_tfidf[sample_idx][fake_indices] == 0]\n",
    "idx_fake = correct_fake[0]\n",
    "\n",
    "shap_exp_fake = shap.Explanation(\n",
    "    values=shap_values[idx_fake],\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_sample[idx_fake].toarray().flatten(),\n",
    "    feature_names=feature_names\n",
    ")\n",
    "shap.waterfall_plot(shap_exp_fake, max_display=15, show=False)\n",
    "plt.title('SHAP Waterfall — Correctly Classified Fake Article', fontweight='bold')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot — one correctly classified REAL article\n",
    "real_indices = np.where(y_test[sample_idx] == 1)[0]\n",
    "correct_real = real_indices[y_pred_tfidf[sample_idx][real_indices] == 1]\n",
    "idx_real = correct_real[0]\n",
    "\n",
    "shap_exp_real = shap.Explanation(\n",
    "    values=shap_values[idx_real],\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_sample[idx_real].toarray().flatten(),\n",
    "    feature_names=feature_names\n",
    ")\n",
    "shap.waterfall_plot(shap_exp_real, max_display=15, show=False)\n",
    "plt.title('SHAP Waterfall — Correctly Classified Real Article', fontweight='bold')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. Save TF-IDF Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(tfidf,     os.path.join(MODELS_DIR, 'tfidf_vectorizer.pkl'))\n",
    "joblib.dump(lr_tfidf,  os.path.join(MODELS_DIR, 'logistic_regression_tfidf.pkl'))\n",
    "print('TF-IDF vectorizer and LR model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Word2Vec Logistic Regression\n",
    "\n",
    "Instead of sparse TF-IDF counts we represent each article as the **mean Word2Vec vector** across all recognised tokens. This gives a dense 300-dimensional representation that captures semantic similarity between words.\n",
    "\n",
    "We load Google's pretrained `word2vec-google-news-300` via `gensim.downloader` (~1.7 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "print('Loading Word2Vec (google-news-300) — this may take a few minutes on first run...')\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "print('Loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def text_to_vector(text: str, wv, dim: int = 300) -> np.ndarray:\n",
    "    tokens = str(text).split()\n",
    "    vecs = [wv[t] for t in tokens if t in wv]\n",
    "    return np.mean(vecs, axis=0) if vecs else np.zeros(dim)\n",
    "\n",
    "print('Vectorising training set...')\n",
    "X_train_w2v = np.array([text_to_vector(t, wv) for t in tqdm(X_train)])\n",
    "print('Vectorising test set...')\n",
    "X_test_w2v  = np.array([text_to_vector(t, wv) for t in tqdm(X_test)])\n",
    "print(f'Shape: {X_train_w2v.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_w2v = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "y_pred_w2v = lr_w2v.predict(X_test_w2v)\n",
    "y_prob_w2v = lr_w2v.predict_proba(X_test_w2v)[:, 1]\n",
    "\n",
    "print('Word2Vec Logistic Regression Results')\n",
    "print(f\"  Accuracy  : {accuracy_score(y_test, y_pred_w2v):.4f}\")\n",
    "print(f\"  Precision : {precision_score(y_test, y_pred_w2v):.4f}\")\n",
    "print(f\"  Recall    : {recall_score(y_test, y_pred_w2v):.4f}\")\n",
    "print(f\"  F1        : {f1_score(y_test, y_pred_w2v):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix & ROC\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "cm_w2v = confusion_matrix(y_test, y_pred_w2v)\n",
    "sns.heatmap(cm_w2v, annot=True, fmt='d', cmap='Oranges', ax=axes[0],\n",
    "            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix — Word2Vec LR', fontweight='bold')\n",
    "\n",
    "fpr_w, tpr_w, _ = roc_curve(y_test, y_prob_w2v)\n",
    "axes[1].plot(fpr_w, tpr_w, lw=2, label=f'AUC = {auc(fpr_w, tpr_w):.4f}')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "axes[1].set_xlabel('FPR'); axes[1].set_ylabel('TPR')\n",
    "axes[1].set_title('ROC Curve — Word2Vec LR', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lr_w2v, os.path.join(MODELS_DIR, 'logistic_regression_w2v.pkl'))\n",
    "print('Word2Vec LR model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. 5-Fold Cross-Validation\n",
    "\n",
    "Cross-validation gives a statistically robust performance estimate by evaluating models across multiple train/test folds rather than a single random split. We report mean ± std for accuracy and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = ['accuracy', 'f1']\n",
    "\n",
    "# TF-IDF pipeline CV\n",
    "tfidf_pipeline = make_pipeline(\n",
    "    TfidfVectorizer(max_features=50_000, ngram_range=(1, 2)),\n",
    "    LogisticRegression(max_iter=1000, random_state=42)\n",
    ")\n",
    "cv_tfidf = cross_validate(tfidf_pipeline, X, y, cv=skf, scoring=scoring, n_jobs=-1)\n",
    "\n",
    "print('TF-IDF LR — 5-Fold CV')\n",
    "print(f\"  Accuracy : {cv_tfidf['test_accuracy'].mean():.4f} ± {cv_tfidf['test_accuracy'].std():.4f}\")\n",
    "print(f\"  F1       : {cv_tfidf['test_f1'].mean():.4f} ± {cv_tfidf['test_f1'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec CV — we vectorise the full dataset once\n",
    "print('Vectorising full dataset for CV (this takes a moment)...')\n",
    "X_all_w2v = np.array([text_to_vector(t, wv) for t in tqdm(X)])\n",
    "\n",
    "cv_w2v = cross_validate(\n",
    "    LogisticRegression(max_iter=1000, random_state=42),\n",
    "    X_all_w2v, y, cv=skf, scoring=scoring, n_jobs=-1\n",
    ")\n",
    "\n",
    "print('Word2Vec LR — 5-Fold CV')\n",
    "print(f\"  Accuracy : {cv_w2v['test_accuracy'].mean():.4f} ± {cv_w2v['test_accuracy'].std():.4f}\")\n",
    "print(f\"  F1       : {cv_w2v['test_f1'].mean():.4f} ± {cv_w2v['test_f1'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise CV results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "\n",
    "for ax, metric, key in zip(axes, ['Accuracy', 'F1'], ['accuracy', 'f1']):\n",
    "    data = {\n",
    "        'TF-IDF LR': cv_tfidf[f'test_{key}'],\n",
    "        'Word2Vec LR': cv_w2v[f'test_{key}'],\n",
    "    }\n",
    "    ax.boxplot(data.values(), labels=data.keys(), patch_artist=True,\n",
    "               boxprops=dict(facecolor='steelblue', color='navy'),\n",
    "               medianprops=dict(color='white', linewidth=2))\n",
    "    ax.set_title(f'5-Fold CV {metric}', fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim(0.8, 1.01)\n",
    "\n",
    "plt.suptitle('Cross-Validation Performance Comparison', fontsize=13)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Error Analysis (TF-IDF Model)\n",
    "\n",
    "Understanding *where* a model fails is as important as knowing its overall accuracy. We examine the misclassified examples from the TF-IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\n",
    "    'text': X_test,\n",
    "    'true': y_test,\n",
    "    'pred': y_pred_tfidf,\n",
    "    'prob_real': y_prob_tfidf\n",
    "})\n",
    "test_df['correct'] = test_df['true'] == test_df['pred']\n",
    "test_df['word_count'] = test_df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "errors = test_df[~test_df['correct']].copy()\n",
    "print(f'Total misclassified: {len(errors):,} out of {len(test_df):,} ({len(errors)/len(test_df):.2%})')\n",
    "\n",
    "label_map = {0: 'Fake', 1: 'Real'}\n",
    "errors['true_label'] = errors['true'].map(label_map)\n",
    "errors['pred_label'] = errors['pred'].map(label_map)\n",
    "\n",
    "display_cols = ['true_label', 'pred_label', 'prob_real', 'text']\n",
    "print('\\n10 Misclassified Examples:')\n",
    "print(errors[display_cols].head(10).to_string(max_colwidth=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article length distribution: misclassified vs correct\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.hist(test_df[test_df['correct']]['word_count'].clip(upper=1500),\n",
    "        bins=50, alpha=0.55, label='Correctly classified', color='steelblue')\n",
    "ax.hist(errors['word_count'].clip(upper=1500),\n",
    "        bins=50, alpha=0.55, label='Misclassified', color='coral')\n",
    "ax.set_title('Article Length: Misclassified vs Correct', fontweight='bold')\n",
    "ax.set_xlabel('Word Count (clipped at 1500)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(f\"Mean length — correct    : {test_df[test_df['correct']]['word_count'].mean():.0f}\")\n",
    "print(f\"Mean length — misclassified: {errors['word_count'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOP = set(stopwords.words('english'))\n",
    "\n",
    "all_error_words = []\n",
    "for text in errors['text']:\n",
    "    tokens = str(text).lower().split()\n",
    "    all_error_words.extend([t.strip(string.punctuation) for t in tokens\n",
    "                            if t.strip(string.punctuation) not in STOP\n",
    "                            and len(t.strip(string.punctuation)) > 2])\n",
    "\n",
    "top_error_words = Counter(all_error_words).most_common(20)\n",
    "words, counts = zip(*top_error_words)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "ax.barh(list(reversed(words)), list(reversed(counts)), color='coral', edgecolor='white')\n",
    "ax.set_title('Top 20 Words in Misclassified Articles', fontweight='bold')\n",
    "ax.set_xlabel('Frequency')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis — Observations\n",
    "\n",
    "1. **Short articles are harder to classify** — misclassified articles have a lower average word count than correctly classified ones, suggesting the model struggles when there is insufficient text context to build reliable TF-IDF feature vectors.\n",
    "\n",
    "2. **Ambiguous language in borderline articles** — many misclassified articles contain vocabulary common to both classes (generic political and world-event language), where the distinctive fake-news linguistic fingerprints are absent.\n",
    "\n",
    "3. **False negatives in real news** — some real Reuters articles misclassified as fake contain unusually emotional or sensationalist language for a newswire story, which the model (correctly, from a vocabulary perspective) associates with fake content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
