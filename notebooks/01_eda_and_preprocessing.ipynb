{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — EDA & Preprocessing\n",
    "\n",
    "This notebook loads the **ISOT Fake News Dataset**, performs exploratory data analysis (EDA) to understand the data's structure and patterns, then applies a full text-preprocessing pipeline and saves the result to `data/processed/cleaned_isot.csv`.\n",
    "\n",
    "**Outline**\n",
    "1. Load & label data  \n",
    "2. EDA — distributions, word frequencies, word clouds, topic breakdowns  \n",
    "3. Preprocessing pipeline  \n",
    "4. Save cleaned data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid', palette='muted')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "# NLTK resources\n",
    "for r in ['punkt', 'punkt_tab', 'stopwords']:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{r}' if 'punkt' in r else f'corpora/{r}')\n",
    "    except LookupError:\n",
    "        nltk.download(r, quiet=True)\n",
    "\n",
    "print('Libraries loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load & Label Data\n",
    "\n",
    "We load `True.csv` (label **1** = real) and `Fake.csv` (label **0** = fake), concatenate them, and create a combined `title_text` column by joining the article title and body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR = '../data/raw'\n",
    "\n",
    "true_df = pd.read_csv(os.path.join(RAW_DIR, 'True.csv'))\n",
    "fake_df = pd.read_csv(os.path.join(RAW_DIR, 'Fake.csv'))\n",
    "\n",
    "true_df['class'] = 1  # real\n",
    "fake_df['class'] = 0  # fake\n",
    "\n",
    "df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "df['title_text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')\n",
    "\n",
    "print(f'Total articles : {len(df):,}')\n",
    "print(f'Real           : {(df[\"class\"]==1).sum():,}')\n",
    "print(f'Fake           : {(df[\"class\"]==0).sum():,}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values per column:')\n",
    "print(df.isnull().sum())\n",
    "print(f'\\nDuplicate rows: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "We explore the dataset from multiple angles: class balance, article length, vocabulary, and subject-topic distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "counts = df['class'].value_counts().sort_index()\n",
    "bars = ax.bar(['Fake (0)', 'Real (1)'], counts.values,\n",
    "              color=[sns.color_palette('muted')[3], sns.color_palette('muted')[0]],\n",
    "              width=0.5, edgecolor='white')\n",
    "for bar, count in zip(bars, counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 150,\n",
    "            f'{count:,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "ax.set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Number of Articles')\n",
    "ax.set_ylim(0, max(counts.values) * 1.15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Article Length Distribution by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['title_text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "colors = {0: sns.color_palette('muted')[3], 1: sns.color_palette('muted')[0]}\n",
    "labels = {0: 'Fake News', 1: 'Real News'}\n",
    "for cls in [0, 1]:\n",
    "    subset = df[df['class'] == cls]['word_count'].clip(upper=2000)\n",
    "    ax.hist(subset, bins=60, alpha=0.55, color=colors[cls], label=labels[cls], edgecolor='none')\n",
    "\n",
    "for cls in [0, 1]:\n",
    "    mean_wc = df[df['class'] == cls]['word_count'].mean()\n",
    "    ax.axvline(mean_wc, color=colors[cls], linestyle='--', linewidth=1.5,\n",
    "               label=f'{labels[cls]} mean ({mean_wc:.0f})')\n",
    "\n",
    "ax.set_title('Article Length Distribution by Class (word count, clipped at 2000)', fontsize=13)\n",
    "ax.set_xlabel('Word Count')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(df.groupby('class')['word_count'].describe().T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Top 20 Most Common Words per Class (excluding stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP = set(stopwords.words('english'))\n",
    "\n",
    "def top_words(series, n=20):\n",
    "    words = []\n",
    "    for text in series:\n",
    "        tokens = str(text).lower().split()\n",
    "        words.extend([t.strip(string.punctuation) for t in tokens\n",
    "                      if t.strip(string.punctuation) not in STOP\n",
    "                      and len(t.strip(string.punctuation)) > 2])\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "fake_top = top_words(df[df['class']==0]['title_text'])\n",
    "real_top = top_words(df[df['class']==1]['title_text'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for ax, top, title, color in zip(\n",
    "        axes,\n",
    "        [fake_top, real_top],\n",
    "        ['Fake News — Top 20 Words', 'Real News — Top 20 Words'],\n",
    "        [sns.color_palette('muted')[3], sns.color_palette('muted')[0]]):\n",
    "    words, counts = zip(*top)\n",
    "    ax.barh(list(reversed(words)), list(reversed(counts)), color=color, edgecolor='white')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Frequency')\n",
    "plt.suptitle('Top 20 Most Common Words (stopwords removed)', fontsize=13, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Word Clouds — Fake vs. Real News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wc(series, stopwords_set, colormap):\n",
    "    text = ' '.join(series.fillna('').tolist())\n",
    "    return WordCloud(\n",
    "        width=700, height=350,\n",
    "        background_color='white',\n",
    "        stopwords=stopwords_set,\n",
    "        max_words=150,\n",
    "        colormap=colormap,\n",
    "        collocations=False\n",
    "    ).generate(text)\n",
    "\n",
    "wc_fake = make_wc(df[df['class']==0]['title_text'], STOP, 'Reds')\n",
    "wc_real = make_wc(df[df['class']==1]['title_text'], STOP, 'Blues')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.imshow(wc_fake, interpolation='bilinear'); ax1.axis('off'); ax1.set_title('Fake News', fontsize=14, fontweight='bold')\n",
    "ax2.imshow(wc_real, interpolation='bilinear'); ax2.axis('off'); ax2.set_title('Real News', fontsize=14, fontweight='bold')\n",
    "plt.suptitle('Word Clouds', fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Subject/Topic Distribution by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_counts = df.groupby(['subject', 'class']).size().unstack(fill_value=0)\n",
    "subject_counts.columns = ['Fake', 'Real']\n",
    "subject_counts = subject_counts.sort_values('Fake', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "subject_counts.plot(\n",
    "    kind='bar', stacked=True, ax=ax,\n",
    "    color=[sns.color_palette('muted')[3], sns.color_palette('muted')[0]],\n",
    "    edgecolor='white'\n",
    ")\n",
    "ax.set_title('Article Count by Subject and Class', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Subject')\n",
    "ax.set_ylabel('Number of Articles')\n",
    "ax.legend(title='Class')\n",
    "plt.xticks(rotation=35, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f. Average Article Length by Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len = df.groupby(['subject', 'class'])['word_count'].mean().unstack()\n",
    "avg_len.columns = ['Avg Length (Fake)', 'Avg Length (Real)']\n",
    "avg_len = avg_len.sort_values('Avg Length (Fake)', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "avg_len.plot(\n",
    "    kind='barh', ax=ax,\n",
    "    color=[sns.color_palette('muted')[3], sns.color_palette('muted')[0]],\n",
    "    edgecolor='white'\n",
    ")\n",
    "ax.set_title('Average Article Length (words) by Subject', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Average Word Count')\n",
    "ax.legend(title='Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Summary — Key Findings\n",
    "\n",
    "1. **Dataset is nearly balanced** — ~21k real articles and ~23k fake articles, so no heavy class-weighting is needed.\n",
    "2. **Real news articles are longer on average** — the Reuters-sourced real articles tend to be structured news reports, while fake articles span a wide length range including very short and very long pieces.\n",
    "3. **Subject categories are strongly segregated** — subjects like `politicsNews` and `worldNews` appear almost exclusively in real news, while `News` and `politics` (different capitalisation) are overwhelmingly fake. This means subject itself would be a near-perfect feature — but we deliberately exclude it to test models on text alone.\n",
    "4. **\"Reuters\" is a dominant signal in real news** — the word clouds and top-word charts reveal that real news is dominated by Reuters byline language; this will be a very strong SHAP feature in the TF-IDF model (and why we strip those tags in preprocessing to avoid leakage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Preprocessing Pipeline\n",
    "\n",
    "We apply the following steps to the `title_text` column:\n",
    "\n",
    "| Step | Why |\n",
    "|---|---|\n",
    "| Strip Reuters tags | Avoid leaking source identity |\n",
    "| Remove URLs, hashtags, mentions | Noise reduction |\n",
    "| Remove \"featured image via...\" | Boilerplate from fake news sites |\n",
    "| Lowercase | Normalise case |\n",
    "| Remove numbers & punctuation | Reduce vocabulary size |\n",
    "| Tokenise (NLTK) | Split into meaningful units |\n",
    "| Remove stopwords | Focus on content words |\n",
    "| PorterStemmer | Conflate morphological variants |\n",
    "| Rejoin | Produce `clean_text` string |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    # 1. Strip Reuters source tags, e.g. \"WASHINGTON (Reuters) -\"\n",
    "    text = re.sub(r'[A-Z\\s]+\\(Reuters\\)\\s*-\\s*', '', text)\n",
    "    # 2. Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # 3. Remove hashtags and mentions\n",
    "    text = re.sub(r'[@#]\\w+', '', text)\n",
    "    # 4. Remove 'featured image via ...' boilerplate\n",
    "    text = re.sub(r'featured image via.*', '', text, flags=re.IGNORECASE)\n",
    "    # 5. Lowercase\n",
    "    text = text.lower()\n",
    "    # 6. Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # 7. Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # 8. Tokenise\n",
    "    tokens = word_tokenize(text)\n",
    "    # 9. Remove stopwords and short tokens\n",
    "    tokens = [t for t in tokens if t not in STOP and len(t) > 2]\n",
    "    # 10. Stem\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Sanity check\n",
    "sample = \"WASHINGTON (Reuters) - The White House said on Tuesday ...\"\n",
    "print('Before:', sample)\n",
    "print('After :', preprocess(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "print('Applying preprocessing pipeline...')\n",
    "df['clean_text'] = df['title_text'].progress_apply(preprocess)\n",
    "\n",
    "# Drop rows that became empty after cleaning\n",
    "empty_mask = df['clean_text'].str.strip() == ''\n",
    "print(f'Empty after cleaning: {empty_mask.sum()}')\n",
    "df = df[~empty_mask].reset_index(drop=True)\n",
    "\n",
    "print(f'Final dataset size: {len(df):,}')\n",
    "df[['clean_text', 'class']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR = '../data/processed'\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "out_path = os.path.join(PROCESSED_DIR, 'cleaned_isot.csv')\n",
    "df[['clean_text', 'title_text', 'class']].to_csv(out_path, index=False)\n",
    "print(f'Saved {len(df):,} rows to {out_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
