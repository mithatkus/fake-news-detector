{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — CNN Text Classification (Word2Vec Embeddings)\n",
    "\n",
    "This notebook builds a **1D Convolutional Neural Network** for fake news detection using pretrained Word2Vec embeddings as the embedding layer. CNNs learn to detect local patterns (n-gram-like features) in the embedding space, making them more expressive than mean-pooled Word2Vec vectors.\n",
    "\n",
    "**Outline**\n",
    "1. Load data  \n",
    "2. Tokenise & pad sequences  \n",
    "3. Build Word2Vec embedding matrix  \n",
    "4. Build and compile CNN model  \n",
    "5. Train with EarlyStopping  \n",
    "6. Plot training curves  \n",
    "7. Evaluate — metrics, confusion matrix, ROC, PR curve  \n",
    "8. LIME explainability  \n",
    "9. Error analysis  \n",
    "10. Save model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "for r in ['punkt', 'punkt_tab', 'stopwords']:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{r}' if 'punkt' in r else f'corpora/{r}')\n",
    "    except LookupError:\n",
    "        nltk.download(r, quiet=True)\n",
    "\n",
    "MODELS_DIR = '../models'\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "print(f'TF version: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/cleaned_isot.csv')\n",
    "print(f'Loaded {len(df):,} rows')\n",
    "\n",
    "X = df['clean_text'].values\n",
    "y = df['class'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f'Train: {len(X_train):,}  Test: {len(X_test):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Tokenise & Pad Sequences\n",
    "\n",
    "We use Keras's `Tokenizer` to build a vocabulary of the top 35,000 words, then encode each article as an integer sequence and pad/truncate to a fixed length of 430 tokens (the ~95th percentile article length in our dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 35_000\n",
    "MAX_LEN   = 430\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)  # fit ONLY on training data\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_test_seq  = pad_sequences(tokenizer.texts_to_sequences(X_test),  maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "vocab_size = min(MAX_WORDS, len(tokenizer.word_index) + 1)\n",
    "print(f'Vocab size  : {vocab_size:,}')\n",
    "print(f'Sequence shape (train): {X_train_seq.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Build Word2Vec Embedding Matrix\n",
    "\n",
    "We load Google News Word2Vec (300d) and build an embedding matrix where row `i` is the Word2Vec vector for the word with tokenizer index `i`. Words not found in Word2Vec are left as zero vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Word2Vec...')\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "EMBED_DIM = 300\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBED_DIM))\n",
    "hits, misses = 0, 0\n",
    "\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if idx >= vocab_size:\n",
    "        continue\n",
    "    if word in wv:\n",
    "        embedding_matrix[idx] = wv[word]\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "coverage = hits / (hits + misses) * 100\n",
    "print(f'Embedding matrix: {embedding_matrix.shape}')\n",
    "print(f'Word2Vec coverage: {hits:,} hits / {hits+misses:,} words ({coverage:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Build CNN Model\n",
    "\n",
    "Architecture:\n",
    "- **Embedding** (pretrained Word2Vec, frozen)\n",
    "- **Conv1D** (128 filters, kernel=3) — detects local trigram patterns\n",
    "- **MaxPooling1D** (pool=5) — reduces sequence length\n",
    "- **Flatten** → **Dense(128, relu)** → **Dropout(0.3)** → **Dense(1, sigmoid)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(vocab_size, embed_dim, embed_matrix, max_len):\n",
    "    inp = layers.Input(shape=(max_len,))\n",
    "    x = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embed_dim,\n",
    "        weights=[embed_matrix],\n",
    "        trainable=False,\n",
    "        name='embedding'\n",
    "    )(inp)\n",
    "    x = layers.Conv1D(128, kernel_size=3, activation='relu', name='conv1d')(x)\n",
    "    x = layers.MaxPooling1D(pool_size=5, name='maxpool')(x)\n",
    "    x = layers.Flatten(name='flatten')(x)\n",
    "    x = layers.Dense(128, activation='relu', name='dense1')(x)\n",
    "    x = layers.Dropout(0.3, name='dropout')(x)\n",
    "    out = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
    "    model = models.Model(inp, out)\n",
    "    return model\n",
    "\n",
    "cnn = build_cnn(vocab_size, EMBED_DIM, embedding_matrix, MAX_LEN)\n",
    "cnn.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Train\n",
    "\n",
    "We train for up to 10 epochs with EarlyStopping (patience=3) monitoring validation loss. The best weights are automatically restored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "history = cnn.fit(\n",
    "    X_train_seq, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history.history['accuracy'],     label='Train', lw=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='Validation', lw=2, linestyle='--')\n",
    "ax1.set_title('Accuracy', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['loss'],     label='Train', lw=2)\n",
    "ax2.plot(history.history['val_loss'], label='Validation', lw=2, linestyle='--')\n",
    "ax2.set_title('Loss', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch'); ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "\n",
    "plt.suptitle('CNN Training History', fontsize=13)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_cnn = cnn.predict(X_test_seq, batch_size=256).flatten()\n",
    "y_pred_cnn = (y_prob_cnn >= 0.5).astype(int)\n",
    "\n",
    "print('CNN Evaluation')\n",
    "print(f\"  Accuracy  : {accuracy_score(y_test, y_pred_cnn):.4f}\")\n",
    "print(f\"  Precision : {precision_score(y_test, y_pred_cnn):.4f}\")\n",
    "print(f\"  Recall    : {recall_score(y_test, y_pred_cnn):.4f}\")\n",
    "print(f\"  F1        : {f1_score(y_test, y_pred_cnn):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_cnn)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[0],\n",
    "            xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix', fontweight='bold')\n",
    "\n",
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob_cnn)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "axes[1].plot(fpr, tpr, lw=2, label=f'AUC = {roc_auc:.4f}')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "axes[1].set_xlabel('FPR'); axes[1].set_ylabel('TPR')\n",
    "axes[1].set_title('ROC Curve', fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# PR\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_prob_cnn)\n",
    "axes[2].plot(rec, prec, lw=2, label=f'PR-AUC = {auc(rec, prec):.4f}')\n",
    "axes[2].set_xlabel('Recall'); axes[2].set_ylabel('Precision')\n",
    "axes[2].set_title('Precision-Recall Curve', fontweight='bold')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.suptitle('CNN — Evaluation Metrics', fontsize=13)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. LIME Explainability\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) explains individual predictions by perturbing the input and fitting a local linear surrogate. Here we use `LimeTextExplainer` which masks out words and observes how the CNN's probability changes — revealing which words drove the prediction.\n",
    "\n",
    "This is particularly compelling because it works on the **black-box** CNN, making its decisions interpretable without requiring model internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "lime_explainer = LimeTextExplainer(class_names=['Fake', 'Real'])\n",
    "\n",
    "def cnn_predict_proba(texts):\n",
    "    seqs = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    preds = cnn.predict(seqs, verbose=0).flatten()\n",
    "    return np.column_stack([1 - preds, preds])\n",
    "\n",
    "print('LIME explainer ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one correctly classified FAKE and one REAL article\n",
    "test_df = pd.DataFrame({'text': X_test, 'true': y_test, 'pred': y_pred_cnn, 'prob': y_prob_cnn})\n",
    "\n",
    "ex_fake_idx = test_df[(test_df['true'] == 0) & (test_df['pred'] == 0)].index[0]\n",
    "ex_real_idx = test_df[(test_df['true'] == 1) & (test_df['pred'] == 1)].index[0]\n",
    "\n",
    "for label, idx in [('FAKE', ex_fake_idx), ('REAL', ex_real_idx)]:\n",
    "    text = test_df.loc[idx, 'text']\n",
    "    print(f'\\n--- {label} Article (first 200 chars) ---')\n",
    "    print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME for a FAKE article\n",
    "fake_text = test_df.loc[ex_fake_idx, 'text']\n",
    "exp_fake = lime_explainer.explain_instance(fake_text, cnn_predict_proba, num_features=12, num_samples=500)\n",
    "\n",
    "fig = exp_fake.as_pyplot_figure()\n",
    "fig.set_size_inches(9, 5)\n",
    "plt.title('LIME Explanation — Correctly Classified FAKE Article', fontweight='bold', pad=10)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME for a REAL article\n",
    "real_text = test_df.loc[ex_real_idx, 'text']\n",
    "exp_real = lime_explainer.explain_instance(real_text, cnn_predict_proba, num_features=12, num_samples=500)\n",
    "\n",
    "fig = exp_real.as_pyplot_figure()\n",
    "fig.set_size_inches(9, 5)\n",
    "plt.title('LIME Explanation — Correctly Classified REAL Article', fontweight='bold', pad=10)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['correct'] = test_df['true'] == test_df['pred']\n",
    "test_df['word_count'] = test_df['text'].apply(lambda x: len(str(x).split()))\n",
    "errors = test_df[~test_df['correct']].copy()\n",
    "\n",
    "print(f'Misclassified: {len(errors):,} / {len(test_df):,} ({len(errors)/len(test_df):.2%})')\n",
    "\n",
    "label_map = {0: 'Fake', 1: 'Real'}\n",
    "errors['true_label'] = errors['true'].map(label_map)\n",
    "errors['pred_label'] = errors['pred'].map(label_map)\n",
    "\n",
    "print('\\n10 Misclassified Examples:')\n",
    "print(errors[['true_label', 'pred_label', 'prob', 'text']].head(10).to_string(max_colwidth=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "# Length distribution\n",
    "ax1.hist(test_df[test_df['correct']]['word_count'].clip(upper=1500),\n",
    "         bins=50, alpha=0.55, label='Correct', color='steelblue')\n",
    "ax1.hist(errors['word_count'].clip(upper=1500),\n",
    "         bins=50, alpha=0.55, label='Misclassified', color='coral')\n",
    "ax1.set_title('Article Length: Correct vs Misclassified', fontweight='bold')\n",
    "ax1.set_xlabel('Word Count'); ax1.set_ylabel('Frequency')\n",
    "ax1.legend()\n",
    "\n",
    "# Top words in misclassified\n",
    "STOP = set(stopwords.words('english'))\n",
    "err_words = []\n",
    "for text in errors['text']:\n",
    "    err_words.extend([t.strip(string.punctuation) for t in str(text).lower().split()\n",
    "                      if t.strip(string.punctuation) not in STOP\n",
    "                      and len(t.strip(string.punctuation)) > 2])\n",
    "top_ew = Counter(err_words).most_common(20)\n",
    "w, c = zip(*top_ew)\n",
    "ax2.barh(list(reversed(w)), list(reversed(c)), color='coral', edgecolor='white')\n",
    "ax2.set_title('Top 20 Words in Misclassified Articles', fontweight='bold')\n",
    "ax2.set_xlabel('Frequency')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis — Observations\n",
    "\n",
    "1. **Short articles remain the hardest** — the CNN, despite learning local n-gram patterns, still underperforms on very short articles where convolutional filters have minimal signal to detect.\n",
    "\n",
    "2. **The CNN makes fewer errors than TF-IDF LR overall**, but its errors are more confident (higher probability on wrong class), suggesting it has learned some spurious n-gram correlations.\n",
    "\n",
    "3. **Cross-class vocabulary confusion** — misclassified articles often mix vocabulary from both classes (e.g., generic political terms appear equally in real and fake news), making local pattern detection insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(MODELS_DIR, 'cnn_model.keras')\n",
    "cnn.save(save_path)\n",
    "print(f'CNN model saved to {save_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
